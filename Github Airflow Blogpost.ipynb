{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Airflow Story: Cleaning & Visualizing our Github Data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through Astronomer’s short but exciting life so far, we’ve changed our stack, product direction, and target market more times than we can count. The flux has definitely caused some stress, but it has ultimately been a major source of growth both for me individually and for our company as a whole.\n",
    "\n",
    "However, all this pivoting has been unequivocally unkind to two things: our cofounder’s hair color (BEFORE AND AFTER) and our Github org.\n",
    "\n",
    "The last few years have left us with 1000 repos and 1000 orgs. As you can imagine, this made it difficult to do any internal github reporting on which repos we still use, who was closing out issues, what type of issues stay open, and how many issues we have left on a milestone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Airflow on Astronomer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lucky for us, we’re a data engineering company with a module designed to solve exactly that type of problem\n",
    "\n",
    "_Apache Airflow_ is a data workflow management system that allows engineers to schedule, deploy and monitor their own data pipes as DAGs (directed acyclic graphs). Built by developers, for developers, it’s based on the principle that ETL is best expressed in code.\n",
    "\n",
    "Running Airflow on Astronomer lets you leverage all of its features without getting bogged down in things that aren’t related to your data. Not only that, but you get access to your own celery workers, Graphana dashboards for monitoring, and Prometheus for alerting.\n",
    "\n",
    "From a developer's perspective, it lets you focus on _using a tool to solve a problem instead of solving a tool's problems._\n",
    "\n",
    "Astronomer Open is our entire platform in a docker container that you can download and hack on locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spinning up Astronomer Open."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "Clone [https://github.com/astronomerio/astronomer](Astronomer Open) for a local, hackable, version of the entire Astronomer Platform - Airflow, Grafana, Promethesus, and so much more.\n",
=======
    "Clone [https://github.com/astronomerio/astronomer](Astronomer Open) for a local, hackable, version of the entire Astronomer Platform - Airflow, Grafana, Prometheus, and so much more.\n",
>>>>>>> fc03acc46fffaeacbc91cd55c19b1464ff13795b
    "\n",
    "Jump into the airflows directory and run the start script pointing to the DAG directory.\n",
    "\n",
    "`cd astronomer/examples/airflow`\n",
    "\n",
    "`./start YOURDIRECTORYHERE`\n",
    "\n",
    "A bunch of stuff will pop up in the command line, including the address where everything will be located \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"astro_open.png\" title=\"Asto Open\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All custom requirements can be stored in a `requirements.txt` file in the directory you're pointed at (an empty one will be initialized when you run start)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Github Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the heavy lifting in Airflow is done with hooks and operators. \n",
    "\n",
    "    **Hooks** are an interface to external systems (APIs, DBs, etc).\n",
    "    **Operators** are units of logic that use hooks to actually do the work.\n",
    "\n",
    "Since the Github API takes HTTP requests, writing the hook is simply going to be wrapping around the code you'd usually write to hit the Github API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## python code for hitting the Github API \n",
    "import requests\n",
    "r = requests.get('https://api.github.com/', headers={'Authorization': 'TOK:TOKEN'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an Airflow Hook, we take a regular request and wrap around some additional logic (i.e. where is the token, what kind of requests are necessary, etc):\n",
    "\n",
    "The request itself and all the heavy lifting is done by the HttpHook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from airflow.hooks.http_hook import HttpHook\n",
    "\n",
    "# Inherit from the Airflow HttpHook.\n",
    "class GithubHook(HttpHook):\n",
    "    \n",
    "    def __init__(self, github_conn_id):\n",
    "        self.github_token = None\n",
    "        conn_id = self.get_connection(github_conn_id)\n",
    "        if conn_id.extra_dejson.get('token'):\n",
    "            # If the token was entered in the extras field in the connections panel, \n",
    "            \n",
    "            self.github_token = conn_id.extra_dejson.get('token')\n",
    "        super().__init__(method='GET', http_conn_id=github_conn_id) # Keeping it to GET requests here.\n",
    "\n",
    "    def get_conn(self, headers):\n",
    "        \"\"\"\n",
    "        Accepts both Basic and Token Authentication.\n",
    "        If a token exists in the \"Extras\" section\n",
    "        with key \"token\", it is passed in the header.\n",
    "\n",
    "        If not, the hook looks for a user name and password.\n",
    "\n",
    "        In either case, it is important to ensure your privacy\n",
    "        settings for your desired authentication method allow\n",
    "        read access to user, org, and repo information.\n",
    "        \"\"\"\n",
    "        if self.github_token:\n",
    "            headers = {'Authorization': 'token {0}'.format(self.github_token)}\n",
    "            session = super().get_conn(headers)\n",
    "            session.auth = None\n",
    "            return session\n",
    "        return super().get_conn(headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When converting this to an Airflow hook, all the credentials can be stored in the Connections panel (from above, airflow spins up at `http://localhost:8080`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"github_connections.png\" title=\"Github Connecitons\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Moving Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that there's an interface to interact with the external system, we need to define what actually needs to be done.\n",
    "\n",
    "We like using Amazon S3 as an intermediary system when working with Redshift - the COPY command makes inserts easy and if a task fails during a data pipeline, the pipeline can restart and pick up where it left off using the data in S3 . \n",
    "\n",
    "So the workflow is going to go from Github to S3 to Redshift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from airflow.utils.decorators import apply_defaults\n",
    "from airflow.models import BaseOperator\n",
    "from ..hooks.github_hook import GithubHook # Import the GithubHook from before\n",
    "from airflow.hooks import S3Hook\n",
    "from flatten_json import flatten\n",
    "import logging\n",
    "import json\n",
    "\n",
    "\n",
    "class GithubToS3Operator(BaseOperator):\n",
    "    \n",
    "    # Define the params - what input is needed for the logic to execute\n",
    "    \"\"\"\n",
    "    Github To S3 Operator\n",
    "    :param github_conn_id:           The Github connection id.\n",
    "    :type github_conn_id:            string\n",
    "    \n",
    "    :param github_org:               The Github organization.\n",
    "    :type github_org:                string\n",
    "    \n",
    "    :param github_repo:              The Github repository. Required for\n",
    "                                     commits, commit_comments, issue_comments,\n",
    "                                     and issues objects.\n",
    "    :type github_repo:               string\n",
    "    \n",
    "    :param github_object:            The desired Github object. The currently\n",
    "                                     supported values are:\n",
    "                                        - commits\n",
    "                                        - commit_comments\n",
    "                                        - issue_comments\n",
    "                                        - issues\n",
    "                                        - members\n",
    "                                        - organizations\n",
    "                                        - pull_requests\n",
    "                                        - repositories\n",
    "    :type github_object:             string\n",
    "    \n",
    "    :param payload:                  The associated github parameters to\n",
    "                                     pass into the object request as\n",
    "                                     keyword arguments.\n",
    "    :type payload:                   dict\n",
    "    \n",
    "    :param s3_conn_id:               The s3 connection id.\n",
    "    :type s3_conn_id:                string\n",
    "    \n",
    "    :param s3_bucket:                The S3 bucket to be used to store\n",
    "                                     the Github data.\n",
    "    :type s3_bucket:                 string\n",
    "    \n",
    "    :param s3_key:                   The S3 key to be used to store\n",
    "                                     the Github data.\n",
    "    :type s3_key:                    string\n",
    "    \"\"\"\n",
    "\n",
    "    template_field = ['s3_key', 'payload']\n",
    "\n",
    "    @apply_defaults\n",
    "    def __init__(self,\n",
    "                 github_conn_id,\n",
    "                 github_org,\n",
    "                 github_object,\n",
    "                 s3_conn_id,\n",
    "                 s3_bucket,\n",
    "                 s3_key,\n",
    "                 github_repo=None,\n",
    "                 payload={},\n",
    "                 *args,\n",
    "                 **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.github_conn_id = github_conn_id\n",
    "        self.github_org = github_org\n",
    "        self.github_repo = github_repo\n",
    "        self.github_object = github_object\n",
    "        self.payload = payload\n",
    "        self.s3_conn_id = s3_conn_id\n",
    "        self.s3_bucket = s3_bucket\n",
    "        self.s3_key = s3_key\n",
    "        if self.github_object.lower() not in ('commits',\n",
    "                                              'commit_comments',\n",
    "                                              'issue_comments',\n",
    "                                              'issues',\n",
    "                                              'members',\n",
    "                                              'organizations',\n",
    "                                              'pull_requests',\n",
    "                                              'repositories'):\n",
    "            raise Exception('Specified Github object not currently supported.')\n",
    "\n",
    "    def execute(self, context):\n",
    "        g = GithubHook(self.github_conn_id)\n",
    "        s3 = S3Hook(self.s3_conn_id)\n",
    "        output = []\n",
    "\n",
    "        if self.github_object not in ('members',\n",
    "                                      'organizations',\n",
    "                                      'repositories'):\n",
    "            if self.github_repo == 'all':\n",
    "                repos = [repo['name'] for repo in\n",
    "                         self.paginate_data(g,\n",
    "                                            self.methodMapper('repositories'))]\n",
    "                for repo in repos:\n",
    "                    output.extend(self.retrieve_data(g, repo=repo))\n",
    "            elif isinstance(self.github_repo, list):\n",
    "                repos = self.github_repo\n",
    "                for repo in repos:\n",
    "                    output.extend(self.retrieve_data(g, repo=repo))\n",
    "            else:\n",
    "                output = self.retrieve_data(g, repo=self.github_repo)\n",
    "        else:\n",
    "            output = self.retrieve_data(g, repo=self.github_repo)\n",
    "        output = '\\n'.join([json.dumps(flatten(record)) for record in output])\n",
    "        s3.load_string(\n",
    "            string_data=output,\n",
    "            key=self.s3_key,\n",
    "            bucket_name=self.s3_bucket,\n",
    "            replace=True\n",
    "        )\n",
    "# Read the rest of the code HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the rest of the Github code [here](https://github.com/airflow-plugins/github_plugin/blob/master/operators/github_to_s3_operator.py). Once the data is in S3, we we use a standard  [S3 to Redshift Operator](https://github.com/airflow-plugins/s3_to_redshift_operator/blob/master/operators/s3_to_redshift.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the DAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the DAG itself can be written to leverage how the hook and operator handle all of the logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from datetime import datetime\n",
    "from airflow.operators.dummy_operator import DummyOperator\n",
    "from airflow.operators import GithubToS3Operator, S3ToRedshiftOperator\n",
    "from airflow.operators.postgres_operator import PostgresOperator\n",
    "\n",
    "default_args = {'owner': 'airflow',\n",
    "                'start_date': datetime(2018, 1, 5),\n",
    "                'email': ['l5t3o4a9m9q9v1w9@astronomerteam.slack.com'], # Send failure alerts to Slack.\n",
    "                'email_on_failure': True,\n",
    "                'email_on_retry': False\n",
    "                }\n",
    "\n",
    "dag = DAG('github_to_redshift',\n",
    "          default_args=default_args,\n",
    "          schedule_interval='@hourly',\n",
    "          catchup=False\n",
    "          )\n",
    "\n",
    "endpoints = [{\"name\": \"commits\",\n",
    "              \"payload\": {},\n",
    "              \"load_type\": \"rebuild\"},\n",
    "             {\"name\": \"issue_comments\",\n",
    "              \"payload\": {\"state\": \"all\"},\n",
    "              \"load_type\": \"rebuild\"},\n",
    "             {\"name\": \"issues\",\n",
    "              \"payload\": {\"state\": \"all\"},\n",
    "              \"load_type\": \"rebuild\"},\n",
    "             {\"name\": \"repositories\",\n",
    "              \"payload\": {},\n",
    "              \"load_type\": \"rebuild\"},\n",
    "             {\"name\": \"members\",\n",
    "              \"payload\": {},\n",
    "              \"load_type\": \"rebuild\"},\n",
    "             {\"name\": \"pull_requests\",\n",
    "              \"payload\": {\"state\": \"all\"},\n",
    "              \"load_type\": \"rebuild\"}]\n",
    "\n",
    "get_individual_issue_counts = \\\n",
    "    \"\"\"\n",
    "    INSERT INTO github.issue_count_by_user\n",
    "    (SELECT login, sum(count) as count, timestamp\n",
    "     FROM\n",
    "            ((SELECT m.login, count(i.id), cast('{{ execution_date + macros.timedelta(hours=-4) }}' as timestamp) as timestamp\n",
    "            FROM github.astronomerio_issues i\n",
    "            JOIN github.astronomerio_members m\n",
    "            ON i.assignee_id = m.id\n",
    "            WHERE i.state = 'open'\n",
    "            GROUP BY m.login\n",
    "            ORDER BY login)\n",
    "        UNION\n",
    "            (SELECT m.login, count(i.id), cast('{{ execution_date + macros.timedelta(hours=-4) }}' as timestamp) as timestamp\n",
    "            FROM github.astronomerio_issues i\n",
    "            JOIN github.astronomerio_members m\n",
    "            ON i.assignee_id = m.id\n",
    "            WHERE i.state = 'open'\n",
    "            GROUP BY m.login\n",
    "            ORDER BY login)\n",
    "        UNION\n",
    "            (SELECT m.login, count(i.id), cast('{{ execution_date + macros.timedelta(hours=-4) }}' as timestamp) as timestamp\n",
    "            FROM github.\"airflow-plugins_issues\" i\n",
    "            JOIN github.\"airflow-plugins_members\" m\n",
    "            ON i.assignee_id = m.id\n",
    "            WHERE i.state = 'open'\n",
    "            GROUP BY m.login\n",
    "            ORDER BY login))\n",
    "    GROUP BY login, timestamp);\n",
    "    \"\"\"\n",
    "\n",
    "copy_params = [\"COMPUPDATE OFF\",\n",
    "               \"STATUPDATE OFF\",\n",
    "               \"JSON 'auto'\",\n",
    "               \"TRUNCATECOLUMNS\",\n",
    "               \"region as 'us-east-1'\"]\n",
    "\n",
    "orgs = [{'name': 'astronomerio',\n",
    "         'github_conn_id': 'astronomerio-github'},\n",
    "        {'name': 'astronomer-integrations',\n",
    "         'github_conn_id': 'astronomer-integrations-github'},\n",
    "        {'name': 'airflow-plugins',\n",
    "         'github_conn_id': 'astronomerio-github'}]\n",
    "\n",
    "with dag:\n",
    "    kick_off_dag = DummyOperator(task_id='kick_off_dag')\n",
    "\n",
    "    github_transforms = PostgresOperator(task_id='github_transforms',\n",
    "                                         sql=get_individual_issue_counts,\n",
    "                                         postgres_conn_id='astronomer-new-redshift')\n",
    "\n",
    "    for endpoint in endpoints:\n",
    "        for org in orgs:\n",
    "            github = GithubToS3Operator(task_id='github_{0}_data_from_{1}_to_s3'.format(endpoint['name'], org['name']),\n",
    "                                        github_conn_id=org['github_conn_id'],\n",
    "                                        github_org=org['name'],\n",
    "                                        github_repo='all',\n",
    "                                        github_object=endpoint['name'],\n",
    "                                        payload=endpoint['payload'],\n",
    "                                        s3_conn_id='astronomer-s3',\n",
    "                                        s3_bucket='astronomer-internal-reporting',\n",
    "                                        s3_key='github/{0}/{1}.json'.format(org['name'], endpoint['name']))\n",
    "\n",
    "            redshift = S3ToRedshiftOperator(task_id='github_{0}_from_{1}_to_redshift'.format(endpoint['name'], org['name']),\n",
    "                                            s3_conn_id='astronomer-s3',\n",
    "                                            s3_bucket='astronomer-internal-reporting',\n",
    "                                            s3_key='github/{0}/{1}.json'.format(org['name'], endpoint['name']),\n",
    "                                            origin_schema='github/{0}_schema.json'.format(endpoint['name']),\n",
    "                                            load_type='rebuild',\n",
    "                                            copy_params=copy_params,\n",
    "                                            redshift_schema='github',\n",
    "                                            table='{0}_{1}'.format(org['name'], endpoint['name']),\n",
    "                                            redshift_conn_id='astronomer-new-redshift'\n",
    "                                            )\n",
    "            \n",
    "            # Define the dependencies.\n",
    "            kick_off_dag >> github >> redshift >> github_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the SQL and some For loops, the DAG is essentially a config file - it says _what_ to do, _when_ to do it, and the order it should be done - all of the _how_ gets stored in the hook and operator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"github_dag.png\" title=\"Github DAG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DummyOperators can help keep DAGs clean. In the DAG above, there's one that starts the DAG, and one that seperates the moving and using of data.\n",
    "\n",
    "This makes the DAG more idempotent and helps with debugging (i.e. I can easily iterate on my SQL in the DAG above once I've gotten all the data into Redshift the way that I like it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The folder structure for the Github DAG looks like:\n",
    "    \n",
    "<img src=\"github_files.png\" title=\"Github File Tree\" />\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ___init__.py_ files in the _hooks_ and _operators_ folders are empty, but the one in the _GithubPlugin_ directory tells Airflow that this is a plugins folder:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from airflow.plugins_manager import AirflowPlugin\n",
    "from GithubPlugin.hooks.github_hook import GithubHook\n",
    "from GithubPlugin.operators.github_to_s3_operator import GithubToS3Operator\n",
    "\n",
    "\n",
    "class GithubPlugin(AirflowPlugin):\n",
    "    name = \"github_plugin\"\n",
    "    operators = [GithubToS3Operator]\n",
    "    hooks = [GithubHook]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Running Locally with Open"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the workflow has been defined, pop over to `https://localport:8080` to see the workflow run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"airflow_ui.png\" title=\"Airflow UI\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty simple workflow where there's not a whole lot that can go wrong. But what if there was a lot more going on, and you needed information on which part of your DAG was using the most resource intensive or what worker utilization looked like?\n",
    "\n",
    "With Prometheus and Grafana bundled with Open, you get all this and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"open_dashboards.png\" title=\"Open Dashboards\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing and Dashboarding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the workflow has been tested all the data is in Redshift, it's ready for visualization and dashboarding. The SQL in the DAG is what we started with, but our dashboard ended up showing a lot more.\n",
    "\n",
    "There are a ton of [great dashboarding tools out there](https://www.astronomer.io/blog/six-open-source-dashboards/), but we decided on Apache Superset because we love all things open source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"github_dashboard.png\" title=\"Github Dashboard\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, our CTO @schnie is leading the way with commits - guess he's CTO for a reason. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We maintain all of our Airflow plugins [here](https://github.com/airflow-plugins) and [have a list of best pracitces that we briefly touched on](https://docs.astronomer.io/v2/apache_airflow/tutorial/best-practices.html).\n",
    "\n",
    "\n",
    "Like what you see? Want to try something yourself? Download [Open Edition](https://github.com/astronomerio/astronomer) and hack on it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
